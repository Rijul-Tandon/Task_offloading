{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating a custom gym environment for offloading process from IOT devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements \n",
    "pip install numpy matplotlib tensorflow==2.13 keras==2.1.3 keras-rl2 gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete,Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> UAVS can be at different height as distance is calculated using all three coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAV():\n",
    "    def __init__(self):\n",
    "        # position is a integer variable\n",
    "        self.position=random.randint(0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileDevice:\n",
    "    def __init__(self, processes, position):  # Take pre-generated data\n",
    "        self.processes = processes\n",
    "        self.position = position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_list=[]\n",
    "'''Initialize the positions of different mobile agents in your current environemnt'''\n",
    "num_mobiles=100\n",
    "for i in range(num_mobiles):\n",
    "    processes = [80 * 10**6 for _ in range(1)]\n",
    "    #position = random.randint(0, 400)\n",
    "    if i % 2 == 0:\n",
    "        position = random.randint(0, 100)\n",
    "    else:\n",
    "        position = random.randint(200, 400)\n",
    "    mobile_list.append((processes, position))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_after_training=[]\n",
    "energy_consumed_after_training=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffloadingEnv(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reward_when_all_offload=0\n",
    "        self.energy_when_all_offload=0\n",
    "        self.time=0\n",
    "        self.energy=0\n",
    "        # Define action space (2 actions: execute locally or offload)\n",
    "        self.action_space = Discrete(2)\n",
    "        # Initializing the Uavs in the environment\n",
    "        self.uav_list = [UAV() for _ in range(1)]\n",
    "        # The state space is the range of distance between uav and mobile device \n",
    "        self.observation_space = Box(low=0, high=400, shape=(1,), dtype=np.float32)\n",
    "        self.state = np.array([0])\n",
    "        print(self.state.shape)\n",
    "        # global mobile_list\n",
    "        self.mobile_list = mobile_list #list to store the data\n",
    "\n",
    "        self.initial_mobile_data=self.mobile_list[:]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward=0\n",
    "        info = {}\n",
    "        mobile_index=0\n",
    "        current_process_size=self.mobile_list[mobile_index].processes[0]\n",
    "        current_process_postion=self.mobile_list[mobile_index].position\n",
    "\n",
    "        # uav = int(self.state[2] % len(self.uav_list))\n",
    "        uav=0\n",
    "\n",
    "        # remove the process sent for execution\n",
    "        self.mobile_list[mobile_index].processes.pop(0)\n",
    "        \n",
    "        # if the agent does not have any other processes then remove the agent\n",
    "        if(len(self.mobile_list[mobile_index].processes)==0):\n",
    "            self.mobile_list.pop(mobile_index)        \n",
    "            \n",
    "        distance=abs(current_process_postion-self.uav_list[uav].position)\n",
    "\n",
    "        # change the current state to the next state\n",
    "        self.state = np.array([distance])\n",
    "\n",
    "        done=False\n",
    "        \n",
    "        B = 10 * 10**6\n",
    "        P_up = 0.1\n",
    "        sigma_squared = 10**-10\n",
    "        P_NLOS = 10**-8\n",
    "        fading_factor = 1\n",
    "        K = 1 * 10**-28\n",
    "    \n",
    "        if distance != 0:\n",
    "            channel_gain = (10**-5) / distance\n",
    "        else:\n",
    "            channel_gain = 1000\n",
    "    \n",
    "        snr = float((P_up * channel_gain * fading_factor) / (sigma_squared + P_NLOS))\n",
    "        snr=round(snr,2)\n",
    "        transmission_rate = float(B * math.log2(1 + snr))\n",
    "        transmission_rate = round(transmission_rate, 2)\n",
    "    \n",
    "        if transmission_rate == 0:\n",
    "            transmission_time = float('inf')\n",
    "        else:\n",
    "            transmission_time = float(current_process_size / transmission_rate)\n",
    "    \n",
    "        calculation_time_UAV = round(current_process_size * 1000 / (4 * 10**9),2)\n",
    "        calculation_time_local = round(current_process_size * 1000 / (2 * 10**9),2)\n",
    "    \n",
    "        energy_offloading = P_up * transmission_time + K * (4 * 10**9)**3 * calculation_time_UAV\n",
    "        energy_local = K * (2 * 10**9)**3 * calculation_time_local\n",
    "    \n",
    "        offload_cost = round(10 * (calculation_time_UAV + transmission_time) + energy_offloading + 1,2)\n",
    "        local_cost = round(10 * calculation_time_local + energy_local,2)\n",
    "\n",
    "        if action == 1:  # Offload\n",
    "            reward=-offload_cost\n",
    "            if offload_cost<local_cost:\n",
    "                print(\"correct execution \")\n",
    "            self.time+=calculation_time_UAV+transmission_time\n",
    "            self.energy+=energy_offloading\n",
    "            print('offload')\n",
    "\n",
    "        if action == 0:  # Local\n",
    "            reward=-local_cost\n",
    "\n",
    "            if(local_cost<offload_cost):\n",
    "                print(\"correct execution\")\n",
    "            self.time+=calculation_time_local\n",
    "            self.energy+=energy_local\n",
    "            print('local')\n",
    "        \n",
    "  \n",
    "        self.reward_when_all_local-=local_cost\n",
    "        self.reward_when_all_offload-=offload_cost\n",
    "        # Check if the episode is done\n",
    "        if len(self.mobile_list)==0:\n",
    "            done=True\n",
    "            return self.state, reward, done, info\n",
    "\n",
    "        if done:\n",
    "            self.state = np.array([0]) # Set to default when done\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset starting state\n",
    "        self.state = np.array([0])\n",
    "        self.mobile_list = []  # Clear the old list\n",
    "        for processes, position in self.initial_mobile_data:\n",
    "            self.mobile_list.append(MobileDevice(processes[:], position))\n",
    "        # ''' Randomly choose a mobile and assign it  a new random position to include the dynamic behaviour of the position of mobile devices '''\n",
    "        # self.mobile_list[random.randint(0,19)].position=random.randint(0,100)\n",
    "        self.reward_when_all_local=0\n",
    "        self.reward_when_all_offload=0\n",
    "        global time_taken_after_training\n",
    "        time_taken_after_training.append(self.time)\n",
    "        global energy_consumed_after_training\n",
    "        energy_consumed_after_training.append(self.energy)\n",
    "        self.time=0\n",
    "        self.energy=0\n",
    "        \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for testing\n",
    "# lets observe the time total and rewards when the agent have learned nothing\n",
    "if __name__ == \"__main__\":\n",
    "    env = OffloadingEnv()\n",
    "    rewards_random=[]\n",
    "    rewards_offload=[]\n",
    "    rewards_local=[]\n",
    "\n",
    "    episodes = 30\n",
    "    for episode in range(1,episodes):\n",
    "        state = env.reset()  # Reset environment at the start of each episode\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action= env.action_space.sample()  # Sample random action\n",
    "            n_state, reward, done, info = env.step(action)  # Step through environment\n",
    "            score += reward  # Accumulate reward\n",
    "        rewards_random.append(score)\n",
    "        rewards_offload.append(env.reward_when_all_offload)\n",
    "        rewards_local.append(env.reward_when_all_local)\n",
    "        \n",
    "        print(f\"Episode: {episode}, Total_reward: {env.reward_when_all_local}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Implementing DQN using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Activation,Concatenate\n",
    "from tensorflow.keras.optimizers import legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states=env.observation_space.shape\n",
    "actions=env.action_space.n\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import __version__\n",
    "import tensorflow as tf\n",
    "tf.keras.__version__ = __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent ,SARSAAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(1, 1)))  # Input is one-dimensional (1, 1)\n",
    "    model.add(Flatten())  # Flatten the (1, 1) input into a single dimension\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer has 2 units for local and offload costs\n",
    "    return model\n",
    "\n",
    "model_dqn=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To get the weights of the entire model\n",
    "#weights = model_dqn.get_weights()\n",
    "\n",
    "# # Print the weights of the model\n",
    "# for i, weight in enumerate(weights):\n",
    "#     print(f\"Layer {i} weights shape: {weight.shape}\")\n",
    "#     print(weight)  # This will print the weights for each layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy(eps=0.05)  # Start with 5% random actions\n",
    "memory = SequentialMemory(limit=1000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# learning_rates = [1e-1,1e-2,1e-3]\n",
    "# for lr in learning_rates:\n",
    "#     model_dqn=build_model()\n",
    "#     dqn = DQNAgent(model=model_dqn, memory=memory, policy=policy,\n",
    "#             nb_actions=actions, nb_steps_warmup=100, \n",
    "#             target_model_update=1e-1, gamma=0)\n",
    "    \n",
    "#     dqn.compile(legacy.Adam(learning_rate=lr), metrics=['mae'])\n",
    "\n",
    "#     try:\n",
    "#         scores = dqn.fit(env, nb_steps=5000, visualize=False, verbose=1)\n",
    "#     except ValueError as e:\n",
    "#         print(\"An error occurred during training:\", e)\n",
    "#         print(\"Environment State:\", env.state)\n",
    "\n",
    "    \n",
    "#     # Extract episode rewards from scores.history\n",
    "#     rewards = scores.history['episode_reward']\n",
    "#     # Plot rewards for the current learning rate\n",
    "#     plt.plot(rewards, label=f'LR={lr}')\n",
    "\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Reward')\n",
    "# plt.title('Rewards per Episode during DQN Training')\n",
    "# plt.legend(loc='lower center', bbox_to_anchor=(0, 1)) \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model_dqn, memory=memory, policy=policy,\n",
    "            nb_actions=actions, nb_steps_warmup=100, \n",
    "            target_model_update=1e-1, gamma=0)\n",
    "dqn.compile(legacy.Adam(learning_rate=1e-2), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.forward(np.array([247]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.forward(np.array([16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn.predict(np.array([[[106]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_after_training=[]\n",
    "energy_consumed_after_training=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scores = dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)\n",
    "except ValueError as e:\n",
    "    print(\"An error occurred during training:\", e)\n",
    "    # Debugging information\n",
    "    print(\"Environment State:\", env.state)\n",
    "# After training the DQN agent\n",
    "episode_rewards_dqn_train = scores.history['episode_reward']\n",
    "plt.plot(episode_rewards_dqn_train)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_taken_after_training)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Time Taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energy_consumed_after_training)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Energy Consumed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(\"Test Scores:\", scores)\n",
    "episode_rewards_dqn = scores.history['episode_reward']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Implementing Sarsa using Keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [1e-1,1e-2,1e-3]\n",
    "\n",
    "# for lr in learning_rates:\n",
    "#     model_sarsa=build_model()\n",
    "#     sarsa = SARSAAgent(model=model_sarsa, nb_actions=actions, nb_steps_warmup=10, policy=policy,gamma=0)\n",
    "#     sarsa.compile(legacy.Adam(lr=lr), metrics=['mae'])\n",
    "\n",
    "#     try:\n",
    "#         scores = sarsa.fit(env, nb_steps=4000, visualize=False, verbose=1)\n",
    "#     except ValueError as e:\n",
    "#         print(\"An error occurred during training:\", e)\n",
    "#         print(\"Environment State:\", env.state)\n",
    "\n",
    "    \n",
    "#     # Extract episode rewards from scores.history\n",
    "#     rewards = scores.history['episode_reward']\n",
    "\n",
    "\n",
    "#     # Plot rewards for the current learning rate\n",
    "#     plt.plot(rewards, label=f'LR={lr}')\n",
    "    \n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Reward')\n",
    "# plt.title('Rewards per Episode during SARSA Training')\n",
    "# plt.legend(loc='lower center', bbox_to_anchor=(0, 1)) \n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_after_training_dqn=time_taken_after_training\n",
    "energy_consumed_after_training_dqn=energy_consumed_after_training\n",
    "time_taken_after_training=[]\n",
    "energy_consumed_after_training=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sarsa=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sarsa = SARSAAgent(model=model_sarsa, nb_actions=actions, nb_steps_warmup=10, policy=policy,gamma=0)\n",
    "sarsa.compile(legacy.Adam(lr=1e-2), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scores=sarsa.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "except ValueError as e:\n",
    "    print(\"An error occurred during training:\", e)\n",
    "    # Debugging information\n",
    "    print(\"Environment State:\", env.state)\n",
    "episode_rewards_sarsa_train= scores.history['episode_reward']\n",
    "plt.plot(episode_rewards_sarsa_train)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energy_consumed_after_training)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Energy Consumed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_taken_after_training)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Time Taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_sarsa=sarsa.test(env, nb_episodes=10, visualize=False)\n",
    "episode_rewards_sarsa = history_sarsa.history['episode_reward']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards_sarsa_train,label='sarsa_training')\n",
    "plt.plot(episode_rewards_dqn_train,label='DQN_training')\n",
    "plt.plot(rewards_local,label='local execute')\n",
    "plt.plot(rewards_random,label='Random_rewards')\n",
    "plt.plot(rewards_offload,label='offload')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(rewards_local,label='local execute')\n",
    "# plt.plot(rewards_random,label='Random_rewards')\n",
    "# plt.plot(rewards_offload,label='offload')\n",
    "# plt.plot(episode_rewards_dqn,label='DQN')\n",
    "# plt.plot(episode_rewards_sarsa,label='Sarsa')\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"rewards\")\n",
    "# plt.legend(loc='upper left', bbox_to_anchor=(0, 1)) \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances=[]\n",
    "for i in mobile_list:\n",
    "    distances.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# Constants\n",
    "B = 10 * 10**6\n",
    "P_up = 0.1\n",
    "sigma_squared = 10**-10\n",
    "P_NLOS = 10**-8\n",
    "fading_factor = 1\n",
    "K = 1 * 10**-28\n",
    "current_process_size = 80 * 10**6  # Constant process size\n",
    "\n",
    "def calculate_offload(distance):\n",
    "    if distance != 0:\n",
    "        channel_gain = (10**-5) / abs(distance)\n",
    "    else:\n",
    "        channel_gain = 1000\n",
    "\n",
    "    snr = float((P_up * channel_gain * fading_factor) / (sigma_squared + P_NLOS))\n",
    "    snr = round(snr, 2)\n",
    "    transmission_rate = float(B * math.log2(1 + snr))\n",
    "    transmission_rate = round(transmission_rate, 2)\n",
    "\n",
    "    if transmission_rate == 0:\n",
    "        transmission_time = float('inf')\n",
    "    else:\n",
    "        transmission_time = float(current_process_size / transmission_rate)\n",
    "\n",
    "    calculation_time_UAV = round(current_process_size * 1000 / (4 * 10**9), 2)\n",
    "\n",
    "    energy_offloading = P_up * transmission_time + K * (4 * 10**9)**3 * calculation_time_UAV\n",
    "    return calculation_time_UAV + transmission_time, energy_offloading\n",
    "\n",
    "def calculate_local():\n",
    "    calculation_time_local = round(80 * 10**6 * 1000 / (2 * 10**9), 2)\n",
    "    energy_local = K * (2 * 10**9)**3 * calculation_time_local\n",
    "    return calculation_time_local, energy_local\n",
    "\n",
    "\n",
    "time_local = 0\n",
    "energy_local = 0\n",
    "time_offload = 0\n",
    "energy_offload = 0\n",
    "\n",
    "for distance in distances:\n",
    "    local_time, local_energy = calculate_local()\n",
    "    offload_time, offload_energy = calculate_offload(distance)\n",
    "    \n",
    "    time_local += local_time\n",
    "    energy_local += local_energy\n",
    "    time_offload += offload_time\n",
    "    energy_offload += offload_energy\n",
    "\n",
    "# print(\"Time\")\n",
    "print(time_local)\n",
    "# print(time_offload)\n",
    "# print(np.min(time_taken_after_training)) #Make sure time_taken_after_training is defined\n",
    "# print(\"energy consumed\")\n",
    "# print(energy_local)\n",
    "print(energy_offload)\n",
    "# print(np.mean(energy_consumed_after_training)) #Make sure energy_consumed_after_training is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "episodes = range(len(energy_consumed_after_training_dqn))  # Assuming 'energy_consumed_after_training' is defined\n",
    "target_value = energy_offload  # Value for the horizontal line\n",
    "\n",
    "# Plotting\n",
    "plt.plot(episodes, energy_consumed_after_training_dqn, label=\"Energy Consumed\")\n",
    "plt.axhline(y=target_value, color='r', linestyle='--', label=f\"Energy consumed in offloading all process= {target_value}\")\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Energy Consumed\")\n",
    "plt.title(\"Energy Consumed vs Episodes\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = range(len(time_taken_after_training_dqn))  # Assuming 'energy_consumed_after_training' is defined\n",
    "target_value = time_local  # Value for the horizontal line\n",
    "\n",
    "# Plotting\n",
    "plt.plot(episodes, time_taken_after_training_dqn, label=\"Time taken\")\n",
    "plt.axhline(y=target_value, color='r', linestyle='--', label=f\"Time taken in locally executing  all process= {target_value}\")\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Time Taken\")\n",
    "plt.title(\"Time vs Episodes\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offloading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
